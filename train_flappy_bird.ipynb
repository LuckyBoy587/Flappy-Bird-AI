{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a5e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository and install requirements\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Clone the repository (if not already cloned)\n",
    "if not os.path.exists('flappy-bird-assets'):\n",
    "    subprocess.run(['git', 'clone', 'https://github.com/samuelcust/flappy-bird-assets.git'])\n",
    "    os.chdir('flappy-bird-assets')\n",
    "\n",
    "# Install requirements\n",
    "subprocess.run(['pip', 'install', '-r', 'requirements.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c37e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the DQN model\n",
    "from collections import deque\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from flappy_bird_env import FlappyBirdEnv\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "gamma = 0.99  # discount factor\n",
    "epsilon = 1.0  # exploration probability\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "lr = 0.001\n",
    "episodes = 200\n",
    "batch_size = 64\n",
    "memory_size = 2000\n",
    "\n",
    "# --- Environment ---\n",
    "env = FlappyBirdEnv(False)\n",
    "state_dim = env.get_state_size()  # 5\n",
    "action_dim = env.get_action_size()  # 2\n",
    "nn_model_path = \"dqn_flappy_bird.pth\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim: int, action_dim: int):\n",
    "        super(DQN, self).__init__()\n",
    "        # Hidden layers\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        # Output layer\n",
    "        self.out = nn.Linear(64, action_dim)\n",
    "        \n",
    "        # Optional: weight initialization for stability\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.fc2.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity='relu')\n",
    "        nn.init.xavier_uniform_(self.out.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.out(x)   # Q-values for each action\n",
    "\n",
    "def train_dqn():\n",
    "    if nn_model_path and os.path.exists(nn_model_path):\n",
    "        print(f\"Loading model from {nn_model_path}\")\n",
    "        model = DQN(state_dim, action_dim).to(device)\n",
    "        model.load_state_dict(torch.load(nn_model_path, map_location=device))\n",
    "        return model\n",
    "    nn_model = DQN(state_dim, action_dim).to(device)\n",
    "    optimizer = optim.Adam(nn_model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # --- Replay memory ---\n",
    "    memory = deque(maxlen=memory_size)\n",
    "\n",
    "    # --- Îµ-greedy action selection ---\n",
    "    def choose_action(state) -> int:\n",
    "        global epsilon\n",
    "        if random.random() < epsilon:\n",
    "            return random.choice([0, 1])\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                q_values = nn_model(state_tensor)\n",
    "                return int(torch.argmax(q_values).item())\n",
    "            \n",
    "    # --- Training loop ---\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = choose_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            # Train NN if enough samples\n",
    "            if len(memory) >= batch_size:\n",
    "                batch = random.sample(memory, batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                states = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "                actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1).to(device)\n",
    "                rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "                next_states = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
    "                dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "                # Current Q-values for actions taken\n",
    "                q_values = nn_model(states).gather(1, actions)\n",
    "\n",
    "                # Target Q-values\n",
    "                with torch.no_grad():\n",
    "                    q_next = nn_model(next_states).max(1)[0].unsqueeze(1)\n",
    "                    q_target = rewards + gamma * q_next * (1 - dones)\n",
    "\n",
    "                # Loss & backprop\n",
    "                loss = loss_fn(q_values, q_target)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Decay exploration\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "\n",
    "        print(f\"Episode {ep + 1}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "    torch.save(nn_model.state_dict(), nn_model_path)\n",
    "    print(f\"Model saved to {nn_model_path}\")\n",
    "    return nn_model\n",
    "\n",
    "# Train and return the model\n",
    "trained_model = train_dqn()\n",
    "trained_model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
